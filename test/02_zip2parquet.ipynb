{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Introdução\n",
    "\n",
    "Converter de zip para tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow        # Necessário para usar o parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Converter csv to parquet\n",
    "\n",
    "Os arquivos do dados abertos são gigantes e não dá para trabalhar com eles, nos formatos disponibilizados.\n",
    "Logo, é necessário converter para .parquet ou outro formato comprimido.\n",
    "\n",
    "\n",
    "Abaixo tem funções que poderão ser úteis no *dtypes*.\n",
    "```python\n",
    "types_dict = {'A': int, 'B': float}\n",
    "types_dict.update({col: str for col in col_names if col not in types_dict})\n",
    "\n",
    "pd.read_csv('file.csv', dtype=types_dict)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Inicialmente lemos os arquivos, definimos pasta de saída e parâmetros.\n",
    "\n",
    "A função a seguir lê o arquivo zipado, que tem um arquivo *.csv* dentro; pega as colunas e ajusta elas, indicando que o dtype de todas será texto (nesse primeiro momento!)\n",
    "\n",
    "Após isso define o tamanho dos *chunks*! e vai lendo chunk por chunk, inserindo em um arquivo *.parquet*. Ao final, salva esse aquivo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv2parquet(input_file, output_path, encoding, sep):\n",
    "    try:\n",
    "        # Get File\n",
    "        my_zipfile = os.path.basename(input_file)\n",
    "\n",
    "        # Columns Names from csv file\n",
    "        cols = pd.read_csv(\n",
    "            os.path.join(input_file),\n",
    "            sep=sep,\n",
    "            encoding=encoding,\n",
    "            low_memory=False,\n",
    "            nrows=10,\n",
    "            dtype=str, #TODO: Improve dtypes\n",
    "        ).columns\n",
    "\n",
    "        # Set schema from csv file: set all strings\n",
    "        fields = []\n",
    "        for col in list(cols):\n",
    "            col_type = pa.field(col, pa.string()),\n",
    "            fields.append(col_type[0])\n",
    "        my_schema = pa.schema(fields)\n",
    "\n",
    "        # Enumerate chunks to process\n",
    "        df_enum = enumerate(\n",
    "            pd.read_csv(\n",
    "                os.path.join(input_file),\n",
    "                sep=sep,\n",
    "                encoding=encoding,\n",
    "                low_memory=False,\n",
    "                chunksize=10000,\n",
    "                dtype=str,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create Output Directory\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Write parquet in chunks\n",
    "        pqwriter = None\n",
    "        for i, df in enumerate(df_enum):\n",
    "            table = pa.Table.from_pandas(\n",
    "                df[-1],\n",
    "                schema=my_schema,\n",
    "            )\n",
    "\n",
    "            # For the first chunk of records\n",
    "            if i == 0:\n",
    "                # Create a parquet write object giving it an output file\n",
    "                pqwriter = pq.ParquetWriter(\n",
    "                    os.path.join(output_path, '{}.parquet.gzip'.format(my_zipfile.split('.')[0])),\n",
    "                    compression='gzip',\n",
    "                    schema=my_schema,\n",
    "                )\n",
    "            pqwriter.write_table(table)\n",
    "\n",
    "        # Close the parquet writer\n",
    "        pqwriter.close()\n",
    "        print('\"{}\" converter succeed!'.format(my_zipfile))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#input_file = os.path.join(controle_path, 'controle_mensal_parametros_basicos_2020.zip')\n",
    "#output_path = os.path.join(input_path_parquet, 'controle')\n",
    "#encoding = 'ISO-8859-1'\n",
    "#sep = ';'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_csv2parquet(input_file, output_path, encoding, sep)\n",
    "#df = pd.read_parquet(os.path.join(output_path, 'controle_mensal_parametros_basicos_2020.parquet.gzip'))\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Convert *csv* to *parquet*\n",
    "\n",
    "Converte os dados obtidos em formato *csv* (inseridos dentro de um arquivo *zip*) para o formato *parquet*.<br>\n",
    "Nessa primeira transformação não me preocupei com formatos, dtypes, renames etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "paths = ['cadastro', 'controle', 'vigilancia']\n",
    "for path in paths:\n",
    "    # Paths\n",
    "    path_in = os.path.join(bruto_path, path)\n",
    "    path_out = os.path.join(input_path_parquet, path)\n",
    "    \n",
    "    # Loop\n",
    "    list_files = os.listdir(path_in)\n",
    "    for file in list_files:\n",
    "        print('\\n{}'.format(file))\n",
    "        convert_csv2parquet(\n",
    "            os.path.join(path_in, file),\n",
    "            path_out,\n",
    "            encoding='ISO-8859-1',\n",
    "            sep=';',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Repartition\n",
    "\n",
    "Com o formato *parquet*, reparticionei o arquivo para que o acesso fosse facilitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "paths = ['cadastro', 'controle', 'vigilancia']\n",
    "#paths = [path for path in paths if path.startswith('con')]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    # Parameters\n",
    "    path_in = os.path.join(input_path_parquet, path)\n",
    "    path_out = os.path.join(input_path_parquet_partitioned, path)\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    \n",
    "    # Loop\n",
    "    list_files = os.listdir(path_in)\n",
    "    print(list_files)\n",
    "    \n",
    "    for file in list_files:\n",
    "        file_out = os.path.basename(file).split('.')[0]\n",
    "        print(file_out)\n",
    "        df = pq.read_table(os.path.join(path_in, file))\n",
    "        \n",
    "        # Rename Columns\n",
    "        cols = df.column_names\n",
    "        df = df.rename_columns([x.strip().title() for x in cols])\n",
    "        \n",
    "        # Save Parquet Partitioned        \n",
    "        pq.write_to_dataset(\n",
    "            df,\n",
    "            root_path=os.path.join(path_out, file_out),\n",
    "            partition_cols=['Uf', 'Código Ibge']\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "9ff1310743fb78083e61eda919b1855ced787e7556b2d9e40aa50770053af88d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "605px",
    "width": "460px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "364px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
